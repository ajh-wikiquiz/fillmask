{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "convert_to_onnx_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkEpBPYusbR1"
      },
      "source": [
        "# Colab notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKryU0H3w30G",
        "outputId": "56e3ef4a-2f12-4e19-9f2f-fe081c1519bc"
      },
      "source": [
        "!pip install --upgrade transformers\n",
        "!pip install --upgrade onnxruntime\n",
        "!pip install --upgrade onnx\n",
        "!pip install torch==1.8.1+cpu torchvision==0.9.1+cpu torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n",
            "Requirement already satisfied, skipping upgrade: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already up-to-date: onnxruntime in /usr/local/lib/python3.7/dist-packages (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf in /usr/local/lib/python3.7/dist-packages (from onnxruntime) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnxruntime) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->onnxruntime) (56.1.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->onnxruntime) (1.15.0)\n",
            "Requirement already up-to-date: onnx in /usr/local/lib/python3.7/dist-packages (1.9.0)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: protobuf in /usr/local/lib/python3.7/dist-packages (from onnx) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from onnx) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnx) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->onnx) (56.1.0)\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.8.1+cpu in /usr/local/lib/python3.7/dist-packages (1.8.1+cpu)\n",
            "Requirement already satisfied: torchvision==0.9.1+cpu in /usr/local/lib/python3.7/dist-packages (0.9.1+cpu)\n",
            "Requirement already satisfied: torchaudio==0.8.1 in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1+cpu) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1+cpu) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.1+cpu) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_4oc_Al82em"
      },
      "source": [
        "## Create ONNX model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgqrUTk5zkc0"
      },
      "source": [
        "from transformers.convert_graph_to_onnx import convert, optimize, quantize\n",
        "from pathlib import Path"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLE4UEkHyWnz",
        "outputId": "1def1cd1-f3d2-4705-e8ae-57209150b955"
      },
      "source": [
        "model_to_use = 'bert-base-cased'\n",
        "tmp_path = Path('./tmp')\n",
        "onnx_output_path = Path('./ml_model')\n",
        "\n",
        "convert(\n",
        "  framework=\"pt\",  ## pt for pytorch\n",
        "  model=model_to_use,\n",
        "  output=tmp_path/f'{model_to_use}.onnx',\n",
        "  opset=13,\n",
        "  pipeline_name = \"fill-mask\",\n",
        ")\n",
        "\n",
        "# Move from tmp to ml_model directory.\n",
        "(tmp_path/f'{model_to_use}.onnx').rename(onnx_output_path/f'{model_to_use}.onnx')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ONNX opset version set to: 13\n",
            "Loading pipeline (model: bert-base-cased, tokenizer: bert-base-cased)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Using framework PyTorch: 1.8.1+cpu\n",
            "Found input input_ids with shape: {0: 'batch', 1: 'sequence'}\n",
            "Found input token_type_ids with shape: {0: 'batch', 1: 'sequence'}\n",
            "Found input attention_mask with shape: {0: 'batch', 1: 'sequence'}\n",
            "Found output output_0 with shape: {0: 'batch', 1: 'sequence'}\n",
            "Ensuring inputs are in correct order\n",
            "position_ids is not present in the generated input list.\n",
            "Generated inputs order: ['input_ids', 'attention_mask', 'token_type_ids']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py:1968: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  input_tensor.shape[chunk_dim] == tensor_shape for input_tensor in input_tensors\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciGMAnFy05dg",
        "outputId": "5061a3d0-a398-4e6c-f660-36a3ae6efe58"
      },
      "source": [
        "!ls -AcFhlt ml_model"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 4.5G\n",
            "-rw-r--r-- 1 root root 499M May 26 01:04 bert-base-cased.onnx\n",
            "-rw-r--r-- 1 root root 348M May 26 01:00 bert-large-cased-optimized-quantized.onnx\n",
            "-rw-r--r-- 1 root root 1.4G May 26 00:59 bert-large-cased-optimized.onnx\n",
            "-rw-r--r-- 1 root root 348M May 26 00:58 bert-large-cased-quantized.onnx\n",
            "-rw-r--r-- 1 root root 1.4G May 26 00:57 bert-large-cased.onnx\n",
            "-rw-r--r-- 1 root root 126M May 26 00:55 bert-base-cased-optimized-quantized.onnx\n",
            "-rw-r--r-- 1 root root 499M May 26 00:55 bert-base-cased-optimized.onnx\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XB2BnQGnzOsk",
        "outputId": "516196b7-3599-40bf-b62b-806d4c84c044"
      },
      "source": [
        "quantize(onnx_output_path/f'{model_to_use}.onnx')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:onnxruntime.quantization.quantize is deprecated.\n",
            "         Please use quantize_static for static quantization, quantize_dynamic for dynamic quantization.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "As of onnxruntime 1.4.0, models larger than 2GB will fail to quantize due to protobuf constraint.\n",
            "This limitation will be removed in the next release of onnxruntime.\n",
            "Quantized model has been written at ml_model/bert-base-cased-quantized.onnx: ✔\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('ml_model/bert-base-cased-quantized.onnx')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk5RDToD3N5I",
        "outputId": "5ea1cb1a-4f14-4ead-dd28-b4362915c05c"
      },
      "source": [
        "optimize(onnx_output_path/f'{model_to_use}.onnx')\n",
        "quantize(onnx_output_path/f'{model_to_use}-optimized.onnx')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimized model has been written at ml_model/bert-base-cased-optimized.onnx: ✔\n",
            "/!\\ Optimized model contains hardware specific operators which might not be portable. /!\\\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:onnxruntime.quantization.quantize is deprecated.\n",
            "         Please use quantize_static for static quantization, quantize_dynamic for dynamic quantization.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "As of onnxruntime 1.4.0, models larger than 2GB will fail to quantize due to protobuf constraint.\n",
            "This limitation will be removed in the next release of onnxruntime.\n",
            "Quantized model has been written at ml_model/bert-base-cased-optimized-quantized.onnx: ✔\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('ml_model/bert-base-cased-optimized-quantized.onnx')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnMAzIgQ036v",
        "outputId": "66ed2407-ba5f-47ce-beae-acdd7290f88e"
      },
      "source": [
        "!ls -AcFhlt ml_model"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 4.7G\n",
            "-rw-r--r-- 1 root root 126M May 26 01:04 bert-base-cased-optimized-quantized.onnx\n",
            "-rw-r--r-- 1 root root 499M May 26 01:04 bert-base-cased-optimized.onnx\n",
            "-rw-r--r-- 1 root root 126M May 26 01:04 bert-base-cased-quantized.onnx\n",
            "-rw-r--r-- 1 root root 499M May 26 01:04 bert-base-cased.onnx\n",
            "-rw-r--r-- 1 root root 348M May 26 01:00 bert-large-cased-optimized-quantized.onnx\n",
            "-rw-r--r-- 1 root root 1.4G May 26 00:59 bert-large-cased-optimized.onnx\n",
            "-rw-r--r-- 1 root root 348M May 26 00:58 bert-large-cased-quantized.onnx\n",
            "-rw-r--r-- 1 root root 1.4G May 26 00:57 bert-large-cased.onnx\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCvwl9A61cDq"
      },
      "source": [
        "## Testing model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6YqKkHb1bgX",
        "outputId": "10447863-408c-4ad9-f1c7-3de4d4bc33af"
      },
      "source": [
        "import numpy as np\n",
        "from transformers import BertTokenizerFast\n",
        "from onnxruntime import ExecutionMode, InferenceSession, SessionOptions\n",
        "\n",
        "model_to_use = model_to_use or 'bert-base-cased'\n",
        "MASK_STR = '[MASK]'\n",
        "\n",
        "# Create the tokenizer.\n",
        "tokenizer = BertTokenizerFast.from_pretrained(model_to_use)\n",
        "\n",
        "# Create the InferenceSession.\n",
        "options = SessionOptions()\n",
        "options.intra_op_num_threads = 1\n",
        "options.execution_mode = ExecutionMode.ORT_SEQUENTIAL\n",
        "session = InferenceSession(\n",
        "  f'./ml_model/{model_to_use}-quantized.onnx',\n",
        "  options,\n",
        "  providers=['CPUExecutionProvider']\n",
        ")\n",
        "\n",
        "\n",
        "# Simulate some HTTP request and response.\n",
        "def some_request(input_sentence: str) -> dict:\n",
        "  if input_sentence.count(MASK_STR) == 0:\n",
        "    return {'error': f'{MASK_STR} is missing from the text'}\n",
        "\n",
        "  return {'data': {'suggestions': fill_mask_onnx(input_sentence)}}\n",
        "\n",
        "\n",
        "def fill_mask_onnx(input_sentence: str, topn: int = 10) -> list:\n",
        "  tokens = tokenizer(input_sentence, return_tensors='np')\n",
        "  output = session.run(None, tokens.__dict__['data'])\n",
        "  token_logits = output[0]\n",
        "\n",
        "  # Get token indices of the masks.\n",
        "  mask_token_indices = np.where(\n",
        "    tokens['input_ids'] == tokenizer.mask_token_id)[1]\n",
        "\n",
        "  # Get the top tokens for each mask.\n",
        "  result = None\n",
        "  for i in range(len(mask_token_indices)):\n",
        "    mask_token_index = mask_token_indices[i:i+1]\n",
        "\n",
        "    mask_token_logits = token_logits[0, mask_token_index, :]\n",
        "    score = np.exp(mask_token_logits) / np.exp(\n",
        "      mask_token_logits).sum(-1, keepdims=True)\n",
        "\n",
        "    top_idx = (-score[0]).argsort()[:topn]\n",
        "    top_values = score[0][top_idx]\n",
        "\n",
        "    current = None\n",
        "    for token, token_score in zip(top_idx.tolist(), top_values.tolist()):\n",
        "      if current is not None:\n",
        "        current = np.append(\n",
        "            current, {'text': tokenizer.decode([token]), 'score': token_score})\n",
        "      else:\n",
        "        current = np.array(\n",
        "            [{'text': tokenizer.decode([token]), 'score': token_score}])\n",
        "\n",
        "    if result is not None:\n",
        "      result = np.append(result, [current], axis=0)\n",
        "    else:\n",
        "      result = np.array([current])\n",
        "\n",
        "  # Transpose and convert back to a regular list so that it can be serialized.\n",
        "  return np.transpose(result).tolist()\n",
        "\n",
        "\n",
        "print(some_request(f'{MASK_STR} {MASK_STR}, also called performance or concert dance, is intended primarily as a spectacle, usually a performance upon a stage by virtuoso dancers. It often tells a story, perhaps using mime, costume and scenery, or else it may simply interpret the musical accompaniment, which is often specially composed. Examples are western ballet and modern dance, Classical Indian dance and Chinese and Japanese song and dance dramas. Most classical forms are centred upon dance alone, but performance dance may also appear in opera and other forms of musical theatre. Participatory dance, on the other hand, whether it be a folk dance, a social dance, a group dance such as a line, circle, chain or square dance, or a partner dance such as is common in Western ballroom dancing, is undertaken primarily for a common purpose, such as social interaction or exercise, or building flexibility of participants rather than to serve any benefit to onlookers. Such dance seldom has any narrative. A group dance and a corps de ballet, a social partner dance and a pas de deux, differ profoundly. Even a solo dance may be undertaken solely for the satisfaction of the dancer. Participatory dancers often all employ the same movements and steps but, for example, in the rave culture of electronic dance music, vast crowds may engage in free dance, uncoordinated with those around them. On the other hand, some cultures lay down strict rules as to the particular dances in which, for example, men, women and children may or must participate.'))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'data': {'suggestions': [[{'text': 'Performance', 'score': 0.4660041630268097}, {'text': 'dance', 'score': 0.9748988151550293}], [{'text': 'Performing', 'score': 0.05453372746706009}, {'text': 'dancing', 'score': 0.012139721773564816}], [{'text': 'Dance', 'score': 0.03462020307779312}, {'text': 'Dance', 'score': 0.004944453947246075}], [{'text': 'Musical', 'score': 0.034068189561367035}, {'text': 'ballet', 'score': 0.0024087349884212017}], [{'text': 'Concert', 'score': 0.02929711528122425}, {'text': 'dances', 'score': 0.0016606670105829835}], [{'text': 'The', 'score': 0.028143607079982758}, {'text': 'music', 'score': 0.000740378862246871}], [{'text': 'Stage', 'score': 0.018190523609519005}, {'text': 'choreography', 'score': 0.0005686341901309788}], [{'text': 'Live', 'score': 0.017869982868433}, {'text': 'work', 'score': 0.00025001788162626326}], [{'text': 'Public', 'score': 0.016925238072872162}, {'text': '##dance', 'score': 0.00023710304230917245}], [{'text': '.', 'score': 0.01687968336045742}, {'text': 'art', 'score': 0.00021809317695442587}]]}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBATwDd2FSlH"
      },
      "source": [
        "## Save to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hTF1EwmFQyh",
        "outputId": "a5d1a5eb-9b89-4bda-b3fe-7b57c1aa3a67"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rL18gy31GBvz",
        "outputId": "1db8ae71-e945-4e95-c8f6-7cd366b2d7e2"
      },
      "source": [
        "!ls /content/drive/MyDrive/ml_models"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert-base-cased-quantized.onnx\t  wikitext-103-raw.model-50.wv.pkl\n",
            "wikitext-103-raw.model-25.wv.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSCXw-NtFU7y"
      },
      "source": [
        "model_to_use = model_to_use or 'bert-base-cased'\n",
        "\n",
        "# Move to drive.\n",
        "!mv ml_model/bert-base-cased-quantized.onnx /content/drive/MyDrive/ml_models"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNAzp0vbGGi9",
        "outputId": "b263fc4a-67f5-4cdb-d654-2b833defd38c"
      },
      "source": [
        "!ls /content/drive/MyDrive/ml_models"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert-base-cased-quantized.onnx\t  wikitext-103-raw.model-50.wv.pkl\n",
            "wikitext-103-raw.model-25.wv.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}